import { Injectable, Logger } from "@nestjs/common";
import { BaseMessage, HumanMessage, AIMessage } from "@langchain/core/messages";
import { BaseChatModel } from "@langchain/core/language_models/chat_models";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import {
  SupportWorkflowStateValues,
  SupportWorkflowStateUtils,
  AgentResponse,
  SupportWorkflowConfigValues,
} from "../graph.state";
import {
  QueryPriority,
  SupportedLanguage,
  SupportErrorType,
  SupportError,
  ConfidenceLevel,
} from "../../common/types";
import { ModelInitializer } from "@flutchai/flutch-sdk";
// import { trackLLMCall } from "@flutchai/flutch-sdk"; // ‚ùå DOES NOT EXIST - commented out

/**
 * EscalationAgent Node - Handles problematic and critical cases
 *
 * Works as an experienced support manager:
 * 1. Analyzes problematic queries and determines problem type
 * 2. Helps user clarify unclear questions
 * 3. Handles critical situations with priority
 * 4. Provides empathetic and professional responses
 * 5. Initiates transfer to human operator when necessary
 * 6. Collects additional information for diagnostics
 */

interface EscalationCategory {
  type:
    | "unclear_query"
    | "critical_issue"
    | "emotional_user"
    | "technical_failure"
    | "complex_case";
  severity: "low" | "medium" | "high" | "critical";
  requiresHuman: boolean;
  estimatedTime: number; // minutes
}

@Injectable()
export class EscalationAgentNode {
  private readonly logger = new Logger(EscalationAgentNode.name);

  constructor(private readonly modelInitializer: ModelInitializer) {}

  /**
   * Execute the escalation agent logic
   */
  async execute(
    state: SupportWorkflowStateValues,
    config: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): Promise<Partial<SupportWorkflowStateValues>> {
    this.logger.log(
      `Escalation agent handling: ${state.input.query.substring(0, 100)}...`
    );

    const startTime = Date.now();

    try {
      // Advance workflow step
      const stepUpdate = SupportWorkflowStateUtils.advanceStep(
        state,
        "escalation_agent"
      );

      // Step 1: Analyze escalation category and severity only
      const escalationAnalysis = await this.analyzeEscalation(
        state.input.query,
        state.input.priority,
        state.routerDecision,
        state.input.language || "en",
        state,
        config
      );

      // Step 2: Create analysis summary without generating full response
      const agentResponse: AgentResponse = {
        content: "", // No content - will be generated by output node
        confidence: this.calculateConfidence(escalationAnalysis),
        sources: ["Escalation Analysis"],
        reasoning: `Analyzed as ${escalationAnalysis.type} with ${escalationAnalysis.severity} severity`,
        metadata: {
          escalationType: escalationAnalysis.type,
          severity: escalationAnalysis.severity,
          requiresHuman: escalationAnalysis.requiresHuman,
          estimatedTime: escalationAnalysis.estimatedTime,
        },
      };

      // Step 3: Determine if human escalation is needed
      const needsHumanEscalation = this.shouldEscalateToHuman(
        escalationAnalysis,
        agentResponse
      );

      // Step 4: Create simple response metadata (no content generation)
      const finalResponse = {
        content: "", // Empty - will be filled by output node
        sources: agentResponse.sources,
        confidence: agentResponse.confidence,
        agentUsed: "escalation" as const,
        processingTime: Date.now() - startTime,
        metadata: {
          reasoning: agentResponse.reasoning,
          escalationType: escalationAnalysis.type,
          severity: escalationAnalysis.severity,
          estimatedResolutionTime: escalationAnalysis.estimatedTime,
          humanEscalationRequired: needsHumanEscalation,
          ...agentResponse.metadata,
        },
      };

      this.logger.log(
        `Escalation agent completed: ${escalationAnalysis.type} (severity: ${escalationAnalysis.severity})`
      );

      // Record usage
      const usageUpdate = await this.recordTokenUsage(
        state,
        startTime,
        escalationAnalysis.type,
        config
      );

      return {
        ...stepUpdate,
        agentResponse,
        finalResponse,
        output: {
          text: finalResponse.content,
          attachments: [],
          metadata: {
            agentUsed: "escalation",
            confidence: agentResponse.confidence,
            escalationType: escalationAnalysis.type,
            severity: escalationAnalysis.severity,
            needsHumanEscalation,
            processingTime: finalResponse.processingTime,
            estimatedResolutionTime: escalationAnalysis.estimatedTime,
          },
        },
        metadata: {
          ...state.metadata,
          escalationAnalysis,
          humanEscalationRequired: needsHumanEscalation,
          escalationTimestamp: new Date().toISOString(),
        },
        usageRecorder: usageUpdate.usageRecorder,
      };
    } catch (error) {
      this.logger.error(
        `Escalation agent failed: ${error.message}`,
        error.stack
      );

      const errorUpdate = SupportWorkflowStateUtils.addError(
        state,
        `Escalation agent failed: ${error.message}`
      );

      // Create fallback response for escalation failures
      const fallbackResponse = this.createFallbackResponse(
        state.input.query,
        state.input.language || "en",
        error.message
      );

      return {
        ...errorUpdate,
        agentResponse: fallbackResponse.agentResponse,
        finalResponse: fallbackResponse.finalResponse,
        output: fallbackResponse.output,
        metadata: {
          ...state.metadata,
          escalationError: {
            type: SupportErrorType.GENERATION_ERROR,
            message: error.message,
            timestamp: new Date().toISOString(),
            recovery: "Fallback response provided, human intervention required",
          } as SupportError,
        },
      };
    }
  }

  /**
   * Get model configuration from config
   */
  private getModelConfig(
    config: LangGraphRunnableConfig<SupportWorkflowConfigValues>,
    isEmpathy: boolean = false
  ): { modelId: string; temperature?: number; maxTokens?: number } {
    const escalationConfig =
      config.configurable?.graphSettings?.escalationAgent;
    if (!escalationConfig) {
      throw new Error(
        "EscalationAgent requires configuration in graphSettings"
      );
    }

    // For empathy model, check if there's a special empathy config
    if (isEmpathy && escalationConfig.empathyLlmConfig) {
      return {
        modelId: escalationConfig.empathyLlmConfig.modelId,
        temperature: escalationConfig.empathyLlmConfig.temperature,
        maxTokens: escalationConfig.empathyLlmConfig.maxTokens,
      };
    }

    // Support both new JSON schema format and legacy llmConfig format
    let modelId: string,
      temperature: number | undefined,
      maxTokens: number | undefined;

    if (escalationConfig.llmConfig) {
      // Legacy format
      ({ modelId, temperature, maxTokens } = escalationConfig.llmConfig);
    } else {
      // New JSON schema format - properties directly under escalationConfig
      modelId = escalationConfig.model;
      temperature = escalationConfig.temperature;
      maxTokens = escalationConfig.maxTokens;

      // Use special empathy temperature if available and this is empathy mode
      if (isEmpathy && escalationConfig.empathyTemperature) {
        temperature = escalationConfig.empathyTemperature;
      }
    }

    if (!modelId) {
      throw new Error(
        "EscalationAgent requires model/modelId in configuration"
      );
    }

    return { modelId, temperature, maxTokens };
  }

  /**
   * Analyze the escalation category and severity
   */
  private async analyzeEscalation(
    query: string,
    priority?: string,
    routerDecision?: any,
    language?: string,
    state?: SupportWorkflowStateValues,
    config?: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): Promise<EscalationCategory> {
    try {
      const systemPrompt = this.getAnalysisPromptFromConfig(config);
      const userPrompt = this.buildAnalysisUserPrompt(
        query,
        priority,
        routerDecision
      );

      const messages: BaseMessage[] = [
        new AIMessage(systemPrompt),
        new HumanMessage(userPrompt),
      ];

      this.logger.debug("Analyzing escalation category");

      // Get model configuration and initialize on-demand
      const modelConfig = this.getModelConfig(config!);
      const model =
        await this.modelInitializer.initializeChatModel(modelConfig);

      // ‚ùå trackLLMCall does not exist - using direct call
      const response = await model.invoke(messages);
      const content = response.content as string;

      // Parse escalation analysis (would use structured output in production)
      const analysis = this.parseEscalationAnalysis(content);

      this.logger.debug(
        `Escalation analysis: ${analysis.type} (${analysis.severity})`
      );

      return analysis;
    } catch (error) {
      this.logger.error(`Escalation analysis failed: ${error.message}`);
      // Fallback to safe defaults
      return {
        type: "complex_case",
        severity: "medium",
        requiresHuman: true,
        estimatedTime: 30,
      };
    }
  }

  /**
   * Handle unclear queries by asking clarifying questions
   */
  private async handleUnclearQuery(
    query: string,
    language: string,
    analysis: EscalationCategory,
    state: SupportWorkflowStateValues,
    config: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): Promise<AgentResponse> {
    try {
      const systemPrompt = this.getClarificationPromptFromConfig(config);
      const userPrompt = `User's unclear query: "${query}"\n\nHelp them clarify what they need.`;

      const messages: BaseMessage[] = [
        new AIMessage(systemPrompt),
        new HumanMessage(userPrompt),
      ];

      // Get model configuration and initialize on-demand
      const modelConfig = this.getModelConfig(config);
      const model =
        await this.modelInitializer.initializeChatModel(modelConfig);

      // ‚ùå trackLLMCall does not exist - using direct call
      const response = await model.invoke(messages);
      const content = response.content as string;

      return {
        content: content,
        confidence: ConfidenceLevel.MEDIUM,
        sources: ["Clarification Assistant"],
        reasoning: "Providing clarification questions for unclear query",
        followUpQuestions: this.generateClarificationQuestions(query, language),
        metadata: {
          responseType: "clarification",
          escalationType: analysis.type,
          needsFollowUp: true,
        },
      };
    } catch (error) {
      this.logger.error(`Unclear query handling failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Handle critical issues with urgency and priority
   */
  private async handleCriticalIssue(
    query: string,
    priority?: string,
    language?: string,
    analysis?: EscalationCategory,
    state?: SupportWorkflowStateValues,
    config?: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): Promise<AgentResponse> {
    try {
      const systemPrompt = this.getCriticalIssuePromptFromConfig(config);
      const userPrompt = `Critical issue: "${query}"\nPriority: ${priority}\n\nProvide immediate assistance.`;

      const messages: BaseMessage[] = [
        new AIMessage(systemPrompt),
        new HumanMessage(userPrompt),
      ];

      // Get model configuration and initialize on-demand
      const modelConfig = this.getModelConfig(config!);
      const model =
        await this.modelInitializer.initializeChatModel(modelConfig);

      // ‚ùå trackLLMCall does not exist - using direct call
      const response = await model.invoke(messages);
      const content = response.content as string;

      return {
        content: content,
        confidence: ConfidenceLevel.HIGH,
        sources: ["Critical Issue Handler"],
        reasoning: "Immediate response to critical issue",
        metadata: {
          responseType: "critical_response",
          escalationType: analysis?.type,
          urgencyLevel: "high",
          requiresFollowUp: true,
        },
      };
    } catch (error) {
      this.logger.error(`Critical issue handling failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Handle emotional users with empathy and professionalism
   */
  private async handleEmotionalUser(
    query: string,
    language: string,
    analysis: EscalationCategory,
    state: SupportWorkflowStateValues,
    config: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): Promise<AgentResponse> {
    try {
      const systemPrompt = this.getEmpathyPromptFromConfig(config);
      const userPrompt = `User appears frustrated/emotional: "${query}"\n\nProvide empathetic support.`;

      const messages: BaseMessage[] = [
        new AIMessage(systemPrompt),
        new HumanMessage(userPrompt),
      ];

      // Get empathy model configuration and initialize on-demand
      const modelConfig = this.getModelConfig(config, true);
      const model =
        await this.modelInitializer.initializeChatModel(modelConfig);

      // ‚ùå trackLLMCall does not exist - using direct call
      const response = await model.invoke(messages);
      const content = response.content as string;

      return {
        content: content,
        confidence: ConfidenceLevel.HIGH,
        sources: ["Empathy Support"],
        reasoning: "Empathetic response to emotional user",
        metadata: {
          responseType: "empathetic_support",
          escalationType: analysis.type,
          emotionalSupport: true,
          tone: "empathetic",
        },
      };
    } catch (error) {
      this.logger.error(`Emotional user handling failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Handle technical failures from other agents
   */
  private async handleTechnicalFailure(
    query: string,
    metadata: any,
    language: string,
    analysis: EscalationCategory,
    state: SupportWorkflowStateValues,
    config: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): Promise<AgentResponse> {
    try {
      const systemPrompt = this.getTechnicalFailurePromptFromConfig(config);
      const userPrompt = `Original query: "${query}"\nTechnical context: ${JSON.stringify(metadata)}\n\nAddress the technical failure.`;

      const messages: BaseMessage[] = [
        new AIMessage(systemPrompt),
        new HumanMessage(userPrompt),
      ];

      // Get model configuration and initialize on-demand
      const modelConfig = this.getModelConfig(config);
      const model =
        await this.modelInitializer.initializeChatModel(modelConfig);

      // ‚ùå trackLLMCall does not exist - using direct call
      const response = await model.invoke(messages);
      const content = response.content as string;

      return {
        content: content,
        confidence: ConfidenceLevel.MEDIUM,
        sources: ["Technical Failure Recovery"],
        reasoning: "Addressing technical system failure",
        metadata: {
          responseType: "technical_recovery",
          escalationType: analysis.type,
          systemFailure: true,
          originalError: metadata,
        },
      };
    } catch (error) {
      this.logger.error(`Technical failure handling failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Handle complex cases that need special attention
   */
  private async handleComplexCase(
    query: string,
    language: string,
    analysis: EscalationCategory,
    state: SupportWorkflowStateValues,
    config: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): Promise<AgentResponse> {
    try {
      const systemPrompt = this.getComplexCasePromptFromConfig(config);
      const userPrompt = `Complex case: "${query}"\n\nProvide comprehensive assistance.`;

      const messages: BaseMessage[] = [
        new AIMessage(systemPrompt),
        new HumanMessage(userPrompt),
      ];

      // Get model configuration and initialize on-demand
      const modelConfig = this.getModelConfig(config);
      const model =
        await this.modelInitializer.initializeChatModel(modelConfig);

      // ‚ùå trackLLMCall does not exist - using direct call
      const response = await model.invoke(messages);
      const content = response.content as string;

      return {
        content: content,
        confidence: ConfidenceLevel.MEDIUM,
        sources: ["Complex Case Handler"],
        reasoning: "Comprehensive response to complex case",
        metadata: {
          responseType: "complex_case",
          escalationType: analysis.type,
          requiresSpecialistAttention: true,
        },
      };
    } catch (error) {
      this.logger.error(`Complex case handling failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Determine if human escalation is needed
   */
  private shouldEscalateToHuman(
    analysis: EscalationCategory,
    response: AgentResponse
  ): boolean {
    // Escalate if explicitly required
    if (analysis.requiresHuman) return true;

    // Escalate critical issues
    if (analysis.severity === "critical") return true;

    // Escalate if agent confidence is very low
    if (response.confidence < ConfidenceLevel.LOW) return true;

    // Escalate technical failures
    if (analysis.type === "technical_failure") return true;

    return false;
  }

  /**
   * Create final response with escalation actions
   */
  private createFinalResponse(
    agentResponse: AgentResponse,
    analysis: EscalationCategory,
    needsHumanEscalation: boolean,
    processingTime: number
  ) {
    let content = agentResponse.content;

    // Add escalation notice if needed
    if (needsHumanEscalation) {
      const language = "ru"; // Would get from context
      const escalationNotice =
        language === "ru"
          ? "\n\nüîÑ Your request has been escalated to a specialist for additional assistance. We will contact you shortly."
          : "\n\nüîÑ Your request has been escalated to a specialist for additional assistance. We will contact you shortly.";

      content += escalationNotice;
    }

    return {
      content,
      sources: agentResponse.sources,
      confidence: agentResponse.confidence,
      agentUsed: "escalation" as const,
      processingTime,
      metadata: {
        reasoning: agentResponse.reasoning,
        escalationType: analysis.type,
        severity: analysis.severity,
        estimatedResolutionTime: analysis.estimatedTime,
        humanEscalationRequired: needsHumanEscalation,
        ...agentResponse.metadata,
      },
    };
  }

  /**
   * Create fallback response for system failures
   */
  private createFallbackResponse(
    query: string,
    language: string,
    errorMessage: string
  ) {
    const isRussian = language === "ru";

    const fallbackContent = isRussian
      ? `Sorry, a system error occurred while processing your request. Your question: "${query}" will be forwarded to our specialists for manual processing. We will contact you shortly.`
      : `Sorry, a system error occurred while processing your request. Your question: "${query}" will be forwarded to our specialists for manual processing. We will contact you shortly.`;

    const agentResponse: AgentResponse = {
      content: fallbackContent,
      confidence: ConfidenceLevel.LOW,
      sources: ["System Fallback"],
      reasoning: "System error fallback response",
      metadata: {
        responseType: "system_fallback",
        originalError: errorMessage,
        requiresManualIntervention: true,
      },
    };

    const finalResponse = {
      content: fallbackContent,
      sources: ["System Fallback"],
      confidence: ConfidenceLevel.LOW,
      agentUsed: "escalation" as const,
      processingTime: 0,
      metadata: {
        systemFailure: true,
        fallbackResponse: true,
      },
    };

    return {
      agentResponse,
      finalResponse,
      output: {
        text: fallbackContent,
        attachments: [],
        metadata: {
          agentUsed: "escalation",
          systemError: true,
          fallback: true,
        },
      },
    };
  }

  // Helper methods for getting prompts from config (English only)
  private getAnalysisPromptFromConfig(
    config?: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): string {
    const escalationConfig =
      config?.configurable?.graphSettings?.escalationAgent;

    if (escalationConfig?.analysisPrompt) {
      return escalationConfig.analysisPrompt;
    }

    // Default fallback prompt (English only)
    return `You are an escalation expert in technical support.

Your task is to analyze the request and determine:
- Problem type (unclear_query, critical_issue, emotional_user, technical_failure, complex_case)
- Severity level (low, medium, high, critical)
- Whether human escalation is required
- Estimated resolution time

Be precise in categorization for proper routing.`;
  }

  private getClarificationPromptFromConfig(
    config?: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): string {
    const escalationConfig =
      config?.configurable?.graphSettings?.escalationAgent;

    if (escalationConfig?.clarificationPrompt) {
      return escalationConfig.clarificationPrompt;
    }

    // Default fallback prompt (English only)
    return `You are a specialist in handling unclear queries.

Your task:
- Understand what the user really wants to know
- Ask specific, helpful clarifying questions
- Guide them toward a clear, actionable question
- Be patient and supportive

Help the user formulate a clear question.`;
  }

  private getCriticalIssuePromptFromConfig(
    config?: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): string {
    const escalationConfig =
      config?.configurable?.graphSettings?.escalationAgent;

    if (escalationConfig?.criticalIssuePrompt) {
      return escalationConfig.criticalIssuePrompt;
    }

    // Default fallback prompt (English only)
    return `You are a critical issue response specialist.

Your task:
- Acknowledge the urgency immediately
- Provide immediate actionable steps
- Gather necessary diagnostic information
- Escalate to human support when appropriate
- Remain calm and professional

Prioritize quick resolution and user confidence.`;
  }

  private getEmpathyPromptFromConfig(
    config?: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): string {
    const escalationConfig =
      config?.configurable?.graphSettings?.escalationAgent;

    if (escalationConfig?.empathyPrompt) {
      return escalationConfig.empathyPrompt;
    }

    // Default fallback prompt (English only)
    return `You are an empathetic support specialist.

Your approach:
- Acknowledge the user's frustration genuinely
- Use calming, understanding language
- Focus on solutions, not blame
- Offer multiple ways to help
- Be patient and supportive throughout

Help de-escalate while solving the problem.`;
  }

  private getTechnicalFailurePromptFromConfig(
    config?: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): string {
    const escalationConfig =
      config?.configurable?.graphSettings?.escalationAgent;

    if (escalationConfig?.technicalFailurePrompt) {
      return escalationConfig.technicalFailurePrompt;
    }

    // Default fallback prompt (English only)
    return `You are a technical failure specialist.

Your task:
- Analyze the technical context and error information
- Provide systematic troubleshooting steps
- Explain what went wrong in simple terms
- Offer preventive measures for the future
- Know when to escalate to engineering

Focus on both immediate fixes and long-term stability.`;
  }

  private getComplexCasePromptFromConfig(
    config?: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): string {
    const escalationConfig =
      config?.configurable?.graphSettings?.escalationAgent;

    if (escalationConfig?.complexCasePrompt) {
      return escalationConfig.complexCasePrompt;
    }

    // Default fallback prompt (English only)
    return `You are a complex case specialist.

Your approach:
- Break down complex issues into manageable parts
- Address each component systematically
- Coordinate between different aspects of the problem
- Maintain clear communication throughout
- Set realistic expectations for resolution

Provide comprehensive assistance while keeping it understandable.`;
  }

  // Legacy methods for backwards compatibility (deprecated)
  private buildAnalysisPrompt(language?: string): string {
    const isRussian = language === "ru";

    if (isRussian) {
      return `You are an escalation expert in technical support.

Your task is to analyze the request and determine:
- Problem type (unclear_query, critical_issue, emotional_user, technical_failure, complex_case)
- Severity (low, medium, high, critical)
- Whether human operator help is needed
- Estimated resolution time

Be precise in categorization for proper routing.`;
    } else {
      return `You are an escalation expert in technical support.

Your task is to analyze the request and determine:
- Problem type (unclear_query, critical_issue, emotional_user, technical_failure, complex_case)
- Severity (low, medium, high, critical)
- Whether human operator help is needed  
- Estimated resolution time

Be precise in categorization for proper routing.`;
    }
  }

  private buildClarificationPrompt(language: string): string {
    const isRussian = language === "ru";

    if (isRussian) {
      return `You are a specialist in handling unclear queries.

Your task:
- Understand what the user actually wants to know
- Ask specific clarifying questions
- Suggest options for what they might have meant
- Be friendly and patient

Help the user formulate a clear question.`;
    } else {
      return `You are a specialist in handling unclear queries.

Your task:
- Understand what the user actually wants to know
- Ask specific clarifying questions
- Suggest options for what they might have meant
- Be friendly and patient

Help the user formulate a clear question.`;
    }
  }

  private buildCriticalIssuePrompt(language?: string): string {
    const isRussian = language === "ru";

    if (isRussian) {
      return `You are a critical incident specialist.

IMPORTANT: User is reporting a critical issue!

Your task:
- Acknowledge the seriousness of the situation
- Provide immediate help if possible
- Clearly explain next steps
- Assure priority handling
- Gather key diagnostic information

Act quickly and professionally!`;
    } else {
      return `You are a critical incident specialist.

IMPORTANT: User is reporting a critical issue!

Your task:
- Acknowledge the seriousness of the situation
- Provide immediate help if possible
- Clearly explain next steps  
- Assure priority handling
- Gather key diagnostic information

Act quickly and professionally!`;
    }
  }

  private buildEmpathyPrompt(language: string): string {
    const isRussian = language === "ru";

    if (isRussian) {
      return `You are an empathetic support specialist.

The user is clearly upset or frustrated.

Your task:
- Show understanding and empathy
- Acknowledge their feelings as valid
- Calm them with professional approach
- Focus on solving the problem
- Assure willingness to help

Tone should be warm, understanding, but professional.`;
    } else {
      return `You are an empathetic support specialist.

The user is clearly upset or frustrated.

Your task:
- Show understanding and empathy
- Acknowledge their feelings as valid
- Calm them with professional approach
- Focus on solving the problem
- Assure willingness to help

Tone should be warm, understanding, but professional.`;
    }
  }

  private buildTechnicalFailurePrompt(language: string): string {
    const isRussian = language === "ru";

    if (isRussian) {
      return `You are a technical system issues specialist.

The system failed while processing the user's request.

Your task:
- Explain what happened in understandable terms
- Apologize for the inconvenience
- Offer alternative ways to get help
- Assure that the issue will be resolved
- Gather information for the technical team

Be honest but reassuring.`;
    } else {
      return `You are a technical system issues specialist.

The system failed while processing the user's request.

Your task:
- Explain what happened in understandable terms
- Apologize for the inconvenience
- Offer alternative ways to get help
- Assure that the issue will be resolved
- Gather information for the technical team

Be honest but reassuring.`;
    }
  }

  private buildComplexCasePrompt(language: string): string {
    const isRussian = language === "ru";

    if (isRussian) {
      return `You are a complex cases specialist.

This request requires special attention and expertise.

Your task:
- Show that you understand the complexity
- Provide maximum help within your capabilities
- Explain that additional time may be required
- Offer interim solutions if available
- Assure careful consideration

Demonstrate expertise and attentiveness.`;
    } else {
      return `You are a complex cases specialist.

This request requires special attention and expertise.

Your task:
- Show that you understand the complexity
- Provide maximum help within your capabilities
- Explain that additional time may be required
- Offer interim solutions if available
- Assure careful consideration

Demonstrate expertise and attentiveness.`;
    }
  }

  private buildAnalysisUserPrompt(
    query: string,
    priority?: string,
    routerDecision?: any
  ): string {
    let prompt = `Query to analyze: "${query}"`;

    if (priority) {
      prompt += `\nPriority: ${priority}`;
    }

    if (routerDecision) {
      prompt += `\nRouter decision: ${routerDecision.selectedAgent} (confidence: ${routerDecision.confidence})`;
      prompt += `\nRouter reasoning: ${routerDecision.reasoning}`;
    }

    prompt += `\n\nAnalyze this escalation case and provide categorization.`;

    return prompt;
  }

  private parseEscalationAnalysis(content: string): EscalationCategory {
    // Simple parsing logic (would be more sophisticated in production)
    const typeMatch = content.match(
      /type[^:]*:\s*(unclear_query|critical_issue|emotional_user|technical_failure|complex_case)/i
    );
    const severityMatch = content.match(
      /severity[^:]*:\s*(low|medium|high|critical)/i
    );
    const humanMatch = content.match(/human[^:]*:\s*(true|false|yes|no)/i);
    const timeMatch = content.match(/time[^:]*:\s*([0-9]+)/i);

    return {
      type:
        (typeMatch?.[1] as
          | "unclear_query"
          | "critical_issue"
          | "emotional_user"
          | "technical_failure"
          | "complex_case") || "complex_case",
      severity:
        (severityMatch?.[1] as "low" | "medium" | "high" | "critical") ||
        "medium",
      requiresHuman: humanMatch
        ? ["true", "yes"].includes(humanMatch[1].toLowerCase())
        : true,
      estimatedTime: timeMatch ? parseInt(timeMatch[1]) : 30,
    };
  }

  private generateClarificationQuestions(
    query: string,
    language: string
  ): string[] {
    const isRussian = language === "ru";

    if (isRussian) {
      return [
        "Could you clarify which specific part of the system you're having trouble with?",
        "What specific task are you trying to accomplish?",
        "Can you describe what exactly happens when you encounter this issue?",
      ];
    } else {
      return [
        "Could you clarify which specific part of the system you're having trouble with?",
        "What specific task are you trying to accomplish?",
        "Can you describe what exactly happens when you encounter this issue?",
      ];
    }
  }

  /**
   * Calculate confidence based on escalation analysis
   */
  private calculateConfidence(analysis: EscalationCategory): number {
    // Lower confidence for unclear queries, higher for clear categorization
    switch (analysis.type) {
      case "unclear_query":
        return 0.3; // Low confidence - needs clarification
      case "critical_issue":
        return 0.8; // High confidence - clear critical issue
      case "emotional_user":
        return 0.7; // Good confidence - emotional state detected
      case "technical_failure":
        return 0.6; // Medium confidence - system issue
      case "complex_case":
        return 0.4; // Lower confidence - complex analysis needed
      default:
        return 0.5; // Default medium confidence
    }
  }

  private async recordTokenUsage(
    state: SupportWorkflowStateValues,
    startTime: number,
    escalationType: string,
    config: LangGraphRunnableConfig<SupportWorkflowConfigValues>
  ): Promise<Partial<SupportWorkflowStateValues>> {
    const processingTime = Date.now() - startTime;

    // Estimate token usage based on escalation type
    const baseTokens = Math.floor(state.input.query.length / 3);
    const escalationMultiplier =
      escalationType === "critical_issue" ? 1.5 : 1.2;
    const estimatedInputTokens = Math.floor(baseTokens * escalationMultiplier);
    const estimatedOutputTokens =
      escalationType === "emotional_user" ? 400 : 300;

    // Get model ID from configuration
    const modelConfig = this.getModelConfig(config);
    const modelId = modelConfig.modelId;

    state.usageRecorder.recordModelExecution({
      nodeId: "escalation_agent",
      timestamp: Date.now(),
      modelId,
      promptTokens: estimatedInputTokens,
      completionTokens: estimatedOutputTokens,
      latencyMs: processingTime,
    });

    return {
      usageRecorder: state.usageRecorder,
    };
  }
}
